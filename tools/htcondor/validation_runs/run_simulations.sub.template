# submit multiple simulations and harvest scaling and job information

executable = ./run_simulations.sh
arguments = $(SHELL) $(FROM_LINE) $(TO_LINE)

should_transfer_files = YES
transfer_input_files = $(SHELL), $(PYTHON_SIMULATION_SCRIPT)
transfer_output_files = $(PLATFORM_TEMPLATE)_$(SHELL)_$(FROM_LINE)_$(TO_LINE).csv.tar.gz
when_to_transfer_output = ON_EXIT

log = logs/log.$(ClusterId).$(ProcId)
output = logs/out.$(ClusterId).$(ProcId)
error = logs/err.$(ClusterId).$(ProcId)

universe = docker
docker_image = registry.cern.ch/docker.io/mhorzela/dcsim:test

# +MaxWalltime = 1800
+JobFlavour = "espresso"
request_cpus = 1
RequestMemory = 1500M
RequestDisk = 1G

x509userproxy = $ENV(X509_USER_PROXY)

PLATFORM_TEMPLATE = sgbatch_validation_template
# WORKLOAD = crown_ttbar_slowjob.json
# DATASET = crown_ttbar_slowjob.json
# XROOTD_BLOCKSIZE = 10000000000
# STORAGE_BUFFER_SIZE = 0
# SEED = 42

SHELL = cloud-8.list
PYTHON_SIMULATION_SCRIPT = run_shell_simulations.py

FROM_LINE = <FROM_LINE>
TO_LINE = <TO_LINE>

queue 
