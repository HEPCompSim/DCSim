# submit multiple simulations and harvest scaling and job information

executable = ./measure_and_simulate.sh
arguments = $(PLATFORM) $(NJOBS) $(BUFFERSIZE)

should_transfer_files = YES
transfer_input_files = ../../../data/platform-files/$(PLATFORM).xml
transfer_output_files = scaling_dump_$(PLATFORM)$(NJOBS)jobs_b$(BUFFERSIZE).txt, $(PLATFORM)$(NJOBS)b$(BUFFERSIZE).csv
when_to_transfer_output = ON_EXIT

log = logs/log.$(ClusterId).$(ProcId)
output = logs/out.$(ClusterId).$(ProcId)
error = logs/err.$(ClusterId).$(ProcId)


accounting_group=cms.production
Requirements = TARGET.ProvidesIO && (TARGET.Machine=?="ms04.etp.kit.edu")
+RemoteJob = True

universe = docker
docker_image = mschnepf/slc7-condocker

+RequestWalltime = (15*$(NJOBS)) + 3600 + (24*3600*NumJobStarts)
request_cpus = 1
RequestMemory = (10*15*$(NJOBS))+(2*4000*NumJobStarts)
periodic_release = (HoldReasonCode == 34)
RequestDisk = 4000000

x509userproxy = $ENV(X509_USER_PROXY)

PLATFORM = ETPbatch_reduced
BUFFERSIZE = infinity
queue NJOBS in (50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950)
# queue NJOBS in (400)
